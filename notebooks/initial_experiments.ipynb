{"cells":[{"cell_type":"markdown","metadata":{"id":"xY9AVVwQap87"},"source":["## SETUP"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17600,"status":"ok","timestamp":1744633354076,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"gsKlZF7nKpAG","outputId":"65cadea0-2aae-404a-e04b-f964c7f6edeb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","\n","# Mount your Google Drive\n","drive.mount('/gdrive', force_remount=True) # Add force_remount=True to refresh credentials"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2121,"status":"ok","timestamp":1744633596679,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"spOqw2u-NAor","outputId":"c31bdbc2-36e3-4f0f-993c-22481e4ed4f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"]}],"source":["! pip install numpy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13277,"status":"ok","timestamp":1744421749249,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"7fRtls4BoaDl","outputId":"b0c58539-42f1-455a-cdb1-a6a71db6966c"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'LLaMA-Factory' already exists and is not an empty directory.\n","Obtaining file:///content/LLaMA-Factory\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n","  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.1,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.48.3)\n","Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.2.0)\n","Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n","Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n","Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.6)\n","Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n","Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.21.0)\n","Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n","Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n","Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n","Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.10.6)\n","Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.115.12)\n","Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.1)\n","Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n","Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n","Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n","Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (14.3.0)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n","Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.14)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.5.1)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.9.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n","Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n","Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n","Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n","Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.7.2)\n","Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n","Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n","Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n","Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n","Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n","Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n","Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n","Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n","Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.5)\n","Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n","Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n","Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.46.1)\n","Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n","Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n","Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.1)\n","Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (2.27.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.1,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n","Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n","Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n","Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (1.7.1)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n","Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n","Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n","Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n","Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n","Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n","Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n","Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n","Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.3.2)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n","Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.4.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n","Building wheels for collected packages: llamafactory\n","  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26294 sha256=083fa9ed4ab843539eb3f5feef9f2d789a5e9f061bb690b0c2e39e5a138fcc98\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-q78j7epe/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n","Successfully built llamafactory\n","Installing collected packages: llamafactory\n","  Attempting uninstall: llamafactory\n","    Found existing installation: llamafactory 0.9.3.dev0\n","    Uninstalling llamafactory-0.9.3.dev0:\n","      Successfully uninstalled llamafactory-0.9.3.dev0\n","Successfully installed llamafactory-0.9.3.dev0\n"]}],"source":["!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n","!cd LLaMA-Factory && pip install -e ."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":118502,"status":"ok","timestamp":1744633500800,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"l_gnfI5uMPaK","outputId":"4df98639-4ad3-48f2-dcf2-13acbb6d487a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.6.0+cu124\n","Uninstalling torch-2.6.0+cu124:\n","  Successfully uninstalled torch-2.6.0+cu124\n","Found existing installation: torchvision 0.21.0+cu124\n","Uninstalling torchvision-0.21.0+cu124:\n","  Successfully uninstalled torchvision-0.21.0+cu124\n","Found existing installation: torchaudio 2.6.0+cu124\n","Uninstalling torchaudio-2.6.0+cu124:\n","  Successfully uninstalled torchaudio-2.6.0+cu124\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch==2.5.1\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (838.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m838.4/838.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchvision==0.20.1\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchaudio==2.5.1\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==3.1.0 (from torch==2.5.1)\n","  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.1.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n","Installing collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.5.1+cu118 torchaudio-2.5.1+cu118 torchvision-0.20.1+cu118 triton-3.1.0\n"]}],"source":["# Step 1: Clean up corrupted torch package\n","!rm -rf /usr/local/lib/python3.11/dist-packages/~orch*\n","\n","# Step 2: Uninstall existing torch\n","!pip uninstall -y torch torchvision torchaudio\n","\n","!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":49543,"status":"ok","timestamp":1744633555188,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"139jkdwRKuBQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"759c0c39-0510-4bda-fb1b-93dbe679f016"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.6/433.6 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.3/264.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.2/376.2 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install -qU transformers==4.48.3 datasets==3.2.0 optimum==1.24.0\n","!pip install -qU json-repair==0.29.1\n","!pip install -qU faker==35.2.0\n","!pip install -qU vllm==0.7.2"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1628,"status":"ok","timestamp":1744633574687,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"pleci2yyKyQf","outputId":"7bb53c29-6070-4ab2-d244-e021936ec1f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","The token `finetuning` has been saved to /root/.cache/huggingface/stored_tokens\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful.\n","The current active token is: `finetuning`\n"]}],"source":["from google.colab import userdata\n","\n","#wandb.login(key=userdata.get('wandb'))\n","hf_token = userdata.get('huggingface')\n","!huggingface-cli login --token {hf_token}"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5004,"status":"ok","timestamp":1744633629500,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"1r835W7pK06k"},"outputs":[],"source":["import json\n","import os\n","from os.path import join\n","import random\n","from tqdm.auto import tqdm\n","import requests\n","\n","from pydantic import BaseModel, Field\n","from typing import List, Optional, Literal\n","from datetime import datetime\n","\n","import json_repair\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n","import torch"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1744633634133,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"3MVsKcpJK5_T"},"outputs":[],"source":["data_dir = \"/gdrive/MyDrive/llm_Finetuning\"\n","base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","\n","device = \"cuda\"\n","torch_dtype = None\n","\n","def parse_json(text):\n","    try:\n","        return json_repair.loads(text)\n","    except:\n","        return None"]},{"cell_type":"markdown","metadata":{"id":"ZWcwmSD9a241"},"source":["## Pretrained model qwen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bgc3HRwSK8xc"},"outputs":[],"source":["story = \"\"\"\n","Discharge Medications:\\n\\nOndansetron 8 mg PO twice daily\\nMetoclopramide 10 mg PO QID\\nCelecoxib 200 mg PO once daily\\n\\nDischarge Diagnosis: Cholecystitis, Gallstones\\n\\nDischarge Condition: Stable\\n\\nDischarge Instructions:\\n\\nFollow up with general surgeon in 1 week\\nContinue medications as prescribed\\nAvoid heavy lifting, bending or straining for 1 week\\nFollow-up Instructions:\\nFollow up with general surgeon in 1 week\\n\\nNote: The above-generated clinical note will include all the relevant\n"," sections and headers as the original given clinical note, but with different patient details\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_dBIYp5K_2B"},"outputs":[],"source":["StoryCategory = Literal[\n","    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n","    \"health\", \"entertainment\", \"science\",\n","    \"not_specified\"\n","]\n","\n","EntityType = Literal[\n","    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n","    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n","]\n","\n","class Entity(BaseModel):\n","    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n","    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n","\n","class NewsDetails(BaseModel):\n","    story_title: str = Field(..., min_length=5, max_length=300,\n","                             description=\"A fully informative and SEO optimized title of the story.\")\n","\n","    story_keywords: List[str] = Field(..., min_items=1,\n","                                      description=\"Relevant keywords associated with the story.\")\n","\n","    story_summary: List[str] = Field(\n","                                    ..., min_items=1, max_items=5,\n","                                    description=\"Summarized key points about the story (1-5 points).\"\n","                                )\n","\n","    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n","\n","    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n","                                        description=\"List of identified entities in the story.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1744433921716,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"hjd6uc30XPz5","outputId":"de422cbf-426b-46d9-e117-ae2fa66bd068"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"Admission Date: [2845-7-15] \\nDate of Birth: [2887-4-22] \\nSex: M\\n\\nService: Neurology\\n\\nChief Complaint: Dizziness, Weakness in right arm\\n\\nMajor Surgical or Invasive Procedure: None\\n\\nHistory of Present Illness: 50 years old patient admitted with a 2-day history of dizziness and weakness in the right arm. Patient has a history of hypertension and hyperlipidemia. No recent trauma or exposure to infection.\\n\\nAllergies: NKDA\\n\\nPast Medical History: Hypertension, Hyperlipidemia\\n\\nSocial History: Non-smoker, non-drinker, office manager, married, with two children\\n\\nFamily History: Father with history of stroke\\n\\nPhysical Exam:\\n\\nGeneral: Well-developed, well-nourished male in no acute distress\\nVital signs: BP 126/78 mmHg, HR 84 bpm, RR 18 breaths per minute, Temp 98.6°F, SpO2 97% on room air\\nCardiovascular: Regular rate and rhythm, no murmurs, rubs, or gallops\\nRespiratory: Clear to auscultation bilaterally\\nAbdomen: Soft, non-tender, non-distended\\nExtremities: No edema, pulses intact\\n\\nPertinent Results:\\n\\nECG: No abnormal ST-T changes\\nChest X-ray: No acute cardiopulmonary abnormalities\\nCT Scan: No evidence of intracerebral hemorrhage or infarction\\n\\nBrief Hospital Course:\\nThe patient was admitted with dizziness and weakness in the right arm. The patient underwent a CT scan of the brain, which was unremarkable. The patient was diagnosed with a transient ischemic attack (TIA) and was started on aspirin and atorvastatin. The patient's dizziness and weakness improved significantly, and the patient was discharged in stable condition.\\n\\nMedications on Admission:\\nAspirin 81 mg daily\\nAtorvastatin 40 mg daily\\n\\nDischarge Medications:\\nAspirin 81 mg daily\\nAtorvastatin 40 mg daily\\n\\nDischarge Diagnosis: Transient ischemic attack (TIA)\\nDischarge Condition Stable\\n\\nDischarge instructions:\\n\\n•Follow up with neurologist in 2 weeks\\n•Continue medications as prescribed\\n•Monitor blood pressure and report any new symptoms\\n\\nFollow-up Instructions: Follow up with neurologist in 2 weeks\\n\\nNote: As an assistant, I have created a new clinical note based on the provided structure and headers. The patient's information, medical history, and pertinent results are fictional, and the medications and diagnosis are random and not based on any real-world data.\""]},"execution_count":92,"metadata":{},"output_type":"execute_result"}],"source":["clinical_notes[82]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3khN4sMgLCDn"},"outputs":[],"source":["#for  one story\n","details_extraction_messages = [\n","    {\n","        \"role\": \"system\",\n","        \"content\": \"\\n\".join([\n","            \"You are an NLP data paraser.\",\n","            \"You will be provided by a text associated with a Pydantic scheme.\",\n","            \"Generate the ouptut in the same story language.\",\n","            \"You have to extract JSON details from text according the Pydantic details.\",\n","            \"Extract details as mentioned in text.\",\n","            \"Do not generate any introduction or conclusion.\"\n","\n","        ])\n","    },\n","    {\n","        \"role\": \"user\",\n","        \"content\": \"\\n\".join([\n","            \"## Story:\",\n","            story.strip(),\n","            \"\",\n","\n","            \"## Pydantic Details:\",\n","            json.dumps(\n","                NewsDetails.model_json_schema(), ensure_ascii=False\n","            ),\n","            \"\",\n","\n","            \"## Story Details:\",\n","            \"```json\"\n","        ])\n","    }\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8347,"status":"ok","timestamp":1744509402332,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"2BPfY46dLFsl","outputId":"adde2362-086e-4542-b819-0a939d9c732c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(\n","    base_model_id,\n","    device_map=\"auto\",\n","    torch_dtype = torch_dtype\n",")\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhjxL_IT2o4Q"},"outputs":[],"source":["finetuned_model_id = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","model.load_adapter(finetuned_model_id)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":809},"executionInfo":{"elapsed":69957,"status":"ok","timestamp":1744415202978,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"ZEGgUy0UPn7p","outputId":"c7a51da9-4245-4d74-ed51-2a395a6f0861"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: torch 2.5.1+cu118\n","Uninstalling torch-2.5.1+cu118:\n","  Successfully uninstalled torch-2.5.1+cu118\n","Found existing installation: torchvision 0.20.1+cu118\n","Uninstalling torchvision-0.20.1+cu118:\n","  Successfully uninstalled torchvision-0.20.1+cu118\n","Found existing installation: torchaudio 2.5.1+cu118\n","Uninstalling torchaudio-2.5.1+cu118:\n","  Successfully uninstalled torchaudio-2.5.1+cu118\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch==2.5.1\n","  Using cached https://download.pytorch.org/whl/cu118/torch-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (838.4 MB)\n","Collecting torchvision==0.20.1\n","  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.20.1%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n","Collecting torchaudio==2.5.1\n","  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.5.1%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2024.9.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.89)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.87)\n","Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.11.3.6)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.3.0.86)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.4.1.48)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.7.5.86)\n","Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.8.86)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.20.1) (11.1.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n","Installing collected packages: torch, torchvision, torchaudio\n","Successfully installed torch-2.5.1+cu118 torchaudio-2.5.1+cu118 torchvision-0.20.1+cu118\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"6a09237e6b994ac6acf00172b27fa0c5","pip_warning":{"packages":["torch","torchgen","torchvision"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip uninstall -y torch torchvision torchaudio\n","!pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu118\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11495,"status":"ok","timestamp":1744509427098,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"-YvaqrC9LISd","outputId":"987ad949-5bc7-4c34-a66f-af6b3c2a2fa2"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]}],"source":["text = tokenizer.apply_chat_template(\n","    details_extraction_messages,\n","    tokenize=False,\n","    add_generation_prompt=True\n",")\n","\n","model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","with torch.no_grad():\n","  generated_ids = model.generate(\n","      model_inputs.input_ids,\n","      max_new_tokens=1024,\n","      do_sample=False, top_k=None, temperature=None, top_p=None,\n","  )\n","\n","\n","generated_ids = [\n","    output_ids[len(input_ids):]\n","    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","]\n","\n","response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1744509431877,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"QC4Xr5HALQkL","outputId":"0d1af6da-3559-4a29-d306-b01f63a681d7"},"outputs":[{"data":{"text/plain":["{'story_title': 'Patient Discharge Information',\n"," 'story_keywords': ['discharge medication',\n","  'cholecystitis',\n","  'gallstones',\n","  'general surgeon',\n","  'follow-up instructions'],\n"," 'story_summary': ['Patient discharged with ondansetron 8mg PO twice daily, metoclopramide 10mg PO QID, celecoxib 200mg PO once daily.',\n","  'Diagnosis: Cholecystitis, Gallstones.',\n","  'Condition: Stable.',\n","  'Instructions: Follow up with general surgeon in 1 week; continue medications as prescribed; avoid heavy lifting, bending or straining for 1 week.'],\n"," 'story_category': 'health',\n"," 'story_entities': [{'entity_value': 'Ondansetron', 'entity_type': 'drug'},\n","  {'entity_value': 'Metoclopramide', 'entity_type': 'drug'},\n","  {'entity_value': 'Celecoxib', 'entity_type': 'drug'},\n","  {'entity_value': 'Cholecystitis', 'entity_type': 'disease'},\n","  {'entity_value': 'Gallstones', 'entity_type': 'disease'},\n","  {'entity_value': 'General Surgeon', 'entity_type': 'professional'}]}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["parse_json(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1744434305240,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"tHJtZZOqYKQ_","outputId":"5ec84706-5cec-4e96-fbdf-04375aef5b99"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'{\\n  \"story_title\": \"Patient Admitted Due to Dehydration and Difficulty Walking\",\\n  \"story_keywords\": [\\n    \"patient\",\\n    \"admitted\",\\n    \"dehydration\",\\n    \"difficulty walking\",\\n    \"hypertension\",\\n    \"diabetes\",\\n    \"falls\",\\n    \"weakness\",\\n    \"dizziness\"\\n  ],\\n  \"story_summary\": [\\n    \"A 75-year-old male patient was admitted to the hospital due to dehydration and difficulty walking.\",\\n    \"He has a history of hypertension and diabetes.\",\\n    \"The patient experienced sudden onset of weakness and dizziness leading to a fall.\",\\n    \"Upon admission, the patient showed leg swelling, cool to touch extremities, and brisk tendon reflexes.\"\\n  ],\\n  \"story_category\": \"health\",\\n  \"story_entities\": [\\n    {\\n      \"entity_value\": \"75-year-old male\",\\n      \"entity_type\": \"person-male\"\\n    },\\n    {\\n      \"entity_value\": \"Dehydration\",\\n      \"entity_type\": \"disease\"\\n    },\\n    {\\n      \"entity_value\": \"Difficulty walking\",\\n      \"entity_type\": \"disease\"\\n    },\\n    {\\n      \"entity_value\": \"Hypertension\",\\n      \"entity_type\": \"disease\"\\n    },\\n    {\\n      \"entity_value\": \"Diabetes Mellitus\",\\n      \"entity_type\": \"disease\"\\n    }\\n  ]\\n}'"]},"execution_count":106,"metadata":{},"output_type":"execute_result"}],"source":["generated_summaries[80]"]},{"cell_type":"markdown","metadata":{"id":"MLFPLyAObAQl"},"source":["#DATA preparation https://huggingface.co/datasets/GeorgiaTech/cnotesum/viewer/default/train?p=1&row=100"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_CZ94AkbI_m"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# This loads the full dataset (can also use split=\"train[:100]\" to get only 100 rows)\n","dataset = load_dataset(\"GeorgiaTech/cnotesum\", split=\"train[:1058]\")\n","\n","# Convert to lists\n","clinical_notes = dataset[\"clinical_notes\"]\n","summaries = dataset[\"summary\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1744419069435,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"xZirLtGieYV4","outputId":"63d67f64-bb8f-47f0-b9fd-8551d9376b36"},"outputs":[{"name":"stdout","output_type":"stream","text":["### Chief Complaint:\n","\n","* Hunger\n","* Frequent Urination\n","* Thirst\n","* Fatigue\n","\n","### History of Present Illness:\n","\n","Ivette731 Ileana390 is a 67-year-old non-hispanic other female. The patient has a history of part-time employment, medication review due, not in labor force, limited social contact, full-time employment, stress, and social isolation.\n","\n","### Social History:\n","\n","* Patient is single\n","* Patient has never smoked\n","* Patient identifies as heterosexual\n","* Patient comes from a middle socioeconomic background\n","* Patient has completed some college courses\n","* Patient currently has Medicare\n","\n","### Allergies:\n","\n","* No Known Allergies\n","\n","### Medications:\n","\n","* Lisinopril 10 mg oral tablet\n","* Ondansetron 2 mg/ml injection\n","* Insulin isophane, human 70 unt/ml / insulin, regular, human 30 unt/ml injectable suspension [humulin]\n","* Insulin, regular, human 100 unt/ml injectable solution\n","* 100 ml Propofol 10 mg/ml injection\n","* Abuse-deterrent 12hr oxycodone hydrochloride 10 mg extended-release oral tablet [oxycontin]\n","* Rocuronium bromide 10 mg/ml injectable solution\n","* 1 ml Heparin sodium, porcine 5000 unt/ml injection\n","* 25 ml Protamine sulfate (usp) 10 mg/ml injection\n","* Tramadol hydrochloride 50 mg oral tablet\n","* Amlodipine 2.5 mg oral tablet\n","* 10 ml Alfentanil 0.5 mg/ml injection\n","* Isoflurane 99.9% inhalation solution\n","* Diazepam 5 mg/ml injectable solution\n","* Cefazolin 2000 mg injection\n","\n","### Assessment and Plan:\n","\n","The patient is presenting with full-time employment and social isolation. The following procedures were conducted:\n","\n","* Medication reconciliation (procedure)\n","* Assessment of health and social care needs (procedure)\n","* Assessment using Morse fall scale (procedure)\n","* Screening for domestic abuse (procedure)\n","* Depression screening (procedure)\n","* Depression screening using patient health questionnaire two-item score (procedure)\n","* Assessment of substance use (procedure)\n","* Assessment using alcohol use disorders identification test - consumption (procedure)\n","\n","The patient was prescribed the following medications:\n","\n","* Insulin isophane, human 70 unt/ml / insulin, regular, human 30 unt/ml injectable suspension [humulin]\n","* Abuse-deterrent 12hr oxycodone hydrochloride 10 mg extended-release oral tablet [oxycontin]\n","* Tramadol hydrochloride 50 mg oral tablet\n","* Lisinopril 10 mg oral tablet\n","* Amlodipine 2.5 mg oral tablet\n","\n","Note: The following information is fictional and for demonstration purposes only. When creating a new data example, it is essential to use accurate and relevant information from the patient's record.\n","The summary is: The patient is a 67-year-old non-hispanic other female who presents with chief complaints of hunger, frequent urination, thirst, and fatigue. Her medical history includes part-time employment and medication reviews. The patient identifies as single and has never smoked, and her social history reveals that she comes from a middle socioeconomic background and has completed some college courses. She currently has Medicare and identifies as heterosexual. The patient does not have any known allergies and takes several medications, including Lisinopril, Ondansetron, and Insulin Isophane/Insulin Regular. The patient was assessed and prescribed further medications, including Tramadol, Amlodipine, and Alfentanil.\n","\n","In conclusion, the patient presents with a range of symptoms that may indicate metabolic disorders or other health issues. Further assessment and testing are necessary to determine the underlying cause of her symptoms and to develop an appropriate treatment plan.\n"]}],"source":["print(clinical_notes[0])\n","print('The summary is:', summaries[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":890870,"status":"error","timestamp":1744422771276,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"j0o2Qkq4fR7D","outputId":"ff066959-887a-4fb4-df87-5bf6ebdde42d"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-fab2a38f2b45>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         generated_ids = model.generate(\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2254\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2256\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3257\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    575\u001b[0m                 )\n\u001b[1;32m    576\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    578\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    260\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["generated_summaries = []\n","\n","\n","for story in clinical_notes:\n","    details_extraction_messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are an NLP data paraser.\",\n","                \"You will be provided by a text associated with a Pydantic scheme.\",\n","                \"Generate the ouptut in the same story language.\",\n","                \"You have to extract JSON details from text according the Pydantic details.\",\n","                \"Extract details as mentioned in text.\",\n","                \"Do not generate any introduction or conclusion.\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"\\n\".join([\n","                \"## Story:\",\n","                story.strip(),\n","                \"\",\n","                \"## Pydantic Details:\",\n","                json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n","                \"\",\n","                \"## Story Details:\",\n","                \"```json\"\n","            ])\n","        }\n","    ]\n","\n","    text = tokenizer.apply_chat_template(\n","        details_extraction_messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(\n","            model_inputs.input_ids,\n","            max_new_tokens=1024,\n","            do_sample=False, top_k=None, temperature=None, top_p=None,\n","        )\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    generated_summaries.append(response)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1744422773745,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"Thzwty0Xls45","outputId":"aea7b1b0-cf75-40d3-df6c-56c19480fd1b"},"outputs":[{"data":{"text/plain":["82"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(generated_summaries)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOTtoqhFs0FG"},"outputs":[],"source":["llm_finetunning_data = []\n","\n","system_message = \"\\n\".join([\n","    \"You are a professional NLP data parser.\",\n","    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n","    \"Do not generate any introduction or conclusion.\"\n","])\n","\n","for story, summary in zip(clinical_notes, generated_summaries):\n","    llm_finetunning_data.append({\n","        \"system\": system_message,\n","        \"instruction\": \"\\n\".join([\n","            \"# Story:\",\n","            story.strip(),  # Ensure no leading/trailing whitespace\n","            \"# Task:\",\n","            \"Extract structured details from the story.\",\n","\n","            \"# Output JSON:\",\n","            \"```json\"\n","        ]),\n","        \"input\": \"\",\n","        \"output\": \"\\n\".join([\n","            \"```json\",\n","            json.dumps(summary_data, ensure_ascii=False, default=str),  # Ensure it's valid JSON\n","            \"```\"\n","        ]),\n","        \"history\": []\n","    })\n","\n","\n","random.Random(101).shuffle(llm_finetunning_data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1744424681101,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"IpMnqihqs47T","outputId":"585dee2d-eab6-45ca-b99a-4ae97d926e98"},"outputs":[{"data":{"text/plain":["82"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["len(llm_finetunning_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1744429034261,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"6mA6eIiMuIoJ","outputId":"6ccdb1ce-601b-470d-f92a-47927054937e"},"outputs":[{"data":{"text/plain":["{'system': 'You are a professional NLP data parser.\\nFollow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\\nDo not generate any introduction or conclusion.',\n"," 'instruction': '# Story:\\nAdmission Date: [2845-11-10] \\nDischarge Date: [2845-12-15]\\nDate of Birth: [2768-3-20]\\nSex: F\\nService: NEURO\\n\\nChief Complaint: Sudden loss of consciousness, confusion\\n\\nMajor Surgical or Invasive Procedure: Craniotomy with excision of intracranial mass\\nHistory of Present Illness: A 42-year-old female presented with sudden loss of consciousness and confusion for the past two days. Patient has a history of headaches and recent onset of weakness on the right side of the body. An MRI scan revealed a large intracranial mass, and the patient underwent emergency craniotomy for tumor resection.\\nAllergies: NKDA\\n\\nPast Medical History: Hypertension, Hyperlipidemia\\nSocial History: Non-smoker, non-drinker, office worker, married, with two children\\n\\nFamily History: Mother with history of hypertension\\n\\nPhysical Exam:\\nGeneral: Well-developed, well-nourished female in no acute distress\\nVital signs: BP 122/75 mmHg, HR 74 bpm, RR 16 breaths per minute, Temp 98.6°F, SpO2 98% on room air\\nCardiovascular: Regular rate and rhythm, no murmurs, rubs, or gallops\\nRespiratory: Clear to auscultation bilaterally\\nAbdomen: Soft, non-tender, non-distended\\nExtremities: No edema, pulses intact\\n\\nPertinent Results:\\n\\nECG: Normal\\nCardiac enzymes: Elevated troponin levels\\nChest X-ray: No acute cardiopulmonary abnormalities\\nBlood Test: White blood cell count 12,000/mm3 with 75% neutrophils, Hemoglobin 12.5 g/dL, and platelet count 150,000/mm3\\n\\nMedications on Admission:\\n\\nAspirin 81 mg daily\\nAtorvastatin 40 mg daily\\nLisinopril 5 mg daily\\nDischarge Medications:\\n\\nAspirin 81 mg daily\\nAtorvastatin 40 mg daily\\nLisinopril 10 mg daily\\nDischarge Diagnosis: Meningioma, Status post craniotomy\\n\\nDischarge Condition: Stable\\n\\nDischarge Instructions:\\n\\nFollow up with neurosurgeon in 2 weeks\\nContinue medications as prescribed\\nMonitor neurological symptoms and report any new changes\\nFollow-up Instructions:\\nFollow up with neurosurgeon in 2 weeks\\n# Task:\\nExtract structured details from the story.\\n# Output JSON:\\n```json',\n"," 'input': '',\n"," 'output': '```json\\n{\"story_title\": \"Sudden Onset of Right Hand Weakness\", \"story_keywords\": [\"sudden onset\", \"right hand weakness\", \"hypertension\", \"hyperlipidemia\", \"neurology\"], \"story_summary\": [\"A 55-year-old female patient presented with sudden onset of weakness in her right hand.\", \"She had a history of hypertension and hyperlipidemia.\", \"There was no significant family history or previous neurological illnesses.\"], \"story_category\": \"health\", \"story_entities\": [{\"entity_value\": \"55-year-old female\", \"entity_type\": \"person-female\"}, {\"entity_value\": \"hypertension\", \"entity_type\": \"disease\"}, {\"entity_value\": \"hyperlipidemia\", \"entity_type\": \"disease\"}]}\\n```',\n"," 'history': []}"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["llm_finetunning_data[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bkVuGF43tAIy"},"outputs":[],"source":["train_sample_sz = 82\n","dataset_size = len(llm_finetunning_data)\n","\n","# Define the split ratio (e.g., 90% for training, 10% for validation)\n","train_sample_sz = int(dataset_size * 0.9)  # 90% of the data for training\n","val_sample_sz = dataset_size - train_sample_sz  # Rest for validation\n","\n","train_ds = llm_finetunning_data[:train_sample_sz]\n","eval_ds = llm_finetunning_data[train_sample_sz:]\n","\n","os.makedirs(join(data_dir, \"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n","\n","with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n","    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n","\n","with open(join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n","    json.dump(eval_ds, dest, ensure_ascii=False, default=str)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1744429540258,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"CRhc41RuvErj","outputId":"d63883fc-9fc7-49dc-9281-5aba22dbae38"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/gdrive/MyDrive/llm_Finetuning/datasets/llamafactory-finetune-data/val.json'"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["join(data_dir, \"datasets\", \"llamafactory-finetune-data\", \"val.json\")"]},{"cell_type":"markdown","metadata":{"id":"su19JmGOtMov"},"source":["##Finetuneing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1744432354041,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"wVzZZ_V3tSfF","outputId":"7de57774-f8ff-448c-ff8d-c5d093747bd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"]}],"source":["%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n","\n","### model\n","model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n","trust_remote_code: true\n","\n","### method\n","stage: sft\n","do_train: true\n","finetuning_type: lora\n","lora_rank: 32\n","lora_target: all\n","\n","### dataset\n","dataset: news_finetune_train\n","eval_dataset: news_finetune_val\n","template: qwen\n","cutoff_len: 3500\n","overwrite_cache: true\n","preprocessing_num_workers: 16\n","\n","### output\n","output_dir: /gdrive/MyDrive/llm_Finetuning/models/\n","logging_steps: 10\n","save_steps: 500\n","plot_loss: true\n","\n","### train\n","per_device_train_batch_size: 1\n","gradient_accumulation_steps: 2  # Reduced to save memory\n","learning_rate: 5.0e-5  # Slightly reduced\n","num_train_epochs: 1.0  # Reduced to avoid overfitting\n","lr_scheduler_type: cosine\n","warmup_ratio: 0.1\n","bf16: true\n","ddp_timeout: 180000000\n","gradient_checkpointing: true  # Enable for memory efficiency\n","\n","### eval\n","per_device_eval_batch_size: 1\n","eval_strategy: steps\n","eval_steps: 100\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":368362,"status":"ok","timestamp":1744432726367,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"ZVem4dgE28Hj","outputId":"ff32f581-a660-4415-ef54-22680f79d586"},"outputs":[{"name":"stdout","output_type":"stream","text":["2025-04-12 04:33:44.465510: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1744432424.486184   65919 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1744432424.492473   65919 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","INFO 04-12 04:33:49 __init__.py:190] Automatically detected platform cuda.\n","[INFO|2025-04-12 04:33:52] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,571 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,572 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,572 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,572 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,572 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,572 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:52,572 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2304] 2025-04-12 04:33:52,936 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|configuration_utils.py:696] 2025-04-12 04:33:53,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:768] 2025-04-12 04:33:53,450 >> Model config Qwen2Config {\n","  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.48.3\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2034] 2025-04-12 04:33:53,577 >> loading file chat_template.jinja from cache at None\n","[INFO|tokenization_utils_base.py:2304] 2025-04-12 04:33:53,945 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","[INFO|2025-04-12 04:33:53] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n","[INFO|2025-04-12 04:33:53] llamafactory.data.loader:143 >> Loading dataset /gdrive/MyDrive/llm_Finetuning/datasets/llamafactory-finetune-data/train.json...\n","Converting format of dataset (num_proc=16): 100% 73/73 [00:00<00:00, 282.62 examples/s]\n","[INFO|2025-04-12 04:33:54] llamafactory.data.loader:143 >> Loading dataset /gdrive/MyDrive/llm_Finetuning/datasets/llamafactory-finetune-data/val.json...\n","num_proc must be <= 9. Reducing num_proc to 9 for dataset of size 9.\n","Converting format of dataset (num_proc=9): 100% 9/9 [00:00<00:00, 51.55 examples/s]\n","Running tokenizer on dataset (num_proc=16): 100% 73/73 [00:03<00:00, 22.45 examples/s]\n","training example:\n","input_ids:\n","[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 15106, 510, 2589, 2728, 2631, 25, 508, 17, 23, 19, 20, 12, 16, 16, 12, 16, 15, 60, 715, 4839, 13891, 2631, 25, 508, 17, 23, 19, 20, 12, 16, 17, 12, 16, 20, 921, 1916, 315, 35027, 25, 508, 17, 22, 21, 23, 12, 18, 12, 17, 15, 921, 19439, 25, 434, 198, 1860, 25, 7856, 52, 1285, 271, 65807, 67438, 25, 328, 37995, 4709, 315, 24875, 11, 21340, 271, 34475, 70112, 476, 758, 77034, 44446, 25, 4553, 5559, 84895, 448, 3438, 1816, 315, 10582, 580, 6576, 530, 3072, 198, 13424, 315, 26642, 12516, 2090, 25, 362, 220, 19, 17, 4666, 6284, 8778, 10449, 448, 10968, 4709, 315, 24875, 323, 21340, 369, 279, 3267, 1378, 2849, 13, 28924, 702, 264, 3840, 315, 54303, 323, 3213, 40980, 315, 23078, 389, 279, 1290, 3108, 315, 279, 2487, 13, 1527, 51360, 8569, 10457, 264, 3460, 10582, 580, 6576, 530, 3072, 11, 323, 279, 8720, 53993, 12851, 1560, 5559, 84895, 369, 35154, 312, 2809, 624, 2403, 2375, 550, 25, 70063, 6352, 271, 50113, 12939, 11099, 25, 38415, 529, 2645, 11, 32732, 33115, 307, 21925, 198, 26317, 11099, 25, 11581, 4668, 10451, 11, 2477, 18643, 41112, 11, 5163, 11864, 11, 12224, 11, 448, 1378, 2841, 271, 15192, 11099, 25, 21043, 448, 3840, 315, 62208, 271, 39253, 32310, 510, 15415, 25, 8325, 47961, 291, 11, 1632, 5279, 413, 3304, 8778, 304, 902, 29783, 34004, 198, 53, 2174, 11929, 25, 29067, 220, 16, 17, 17, 14, 22, 20, 9465, 39, 70, 11, 22299, 220, 22, 19, 97724, 11, 43398, 220, 16, 21, 11486, 82, 817, 9383, 11, 19944, 220, 24, 23, 13, 21, 58472, 11, 3089, 46, 17, 220, 24, 23, 4, 389, 3054, 3720, 198, 5770, 815, 32845, 25, 28800, 4379, 323, 36290, 11, 902, 86551, 1723, 11, 10273, 82, 11, 476, 15369, 53689, 198, 1061, 5565, 5269, 25, 12023, 311, 9421, 92013, 367, 20316, 962, 745, 198, 5830, 5600, 268, 25, 24079, 11, 2477, 2385, 1659, 11, 2477, 87259, 2883, 198, 6756, 1826, 1361, 25, 2308, 1578, 9176, 11, 65457, 34439, 271, 47, 529, 13847, 18099, 1447, 7498, 38, 25, 18437, 198, 5770, 17569, 54967, 25, 96593, 8185, 618, 258, 5866, 198, 34, 6402, 1599, 29530, 25, 2308, 29783, 72051, 453, 360, 54792, 74715, 198, 51486, 3393, 25, 5807, 6543, 2779, 1760, 220, 16, 17, 11, 15, 15, 15, 55180, 18, 448, 220, 22, 20, 4, 25793, 21990, 8669, 11, 32824, 93755, 220, 16, 17, 13, 20, 342, 3446, 43, 11, 323, 11968, 1149, 1760, 220, 16, 20, 15, 11, 15, 15, 15, 55180, 18, 271, 13310, 292, 804, 389, 62346, 1447, 2121, 5565, 258, 220, 23, 16, 13742, 7298, 198, 1655, 269, 85, 559, 14768, 220, 19, 15, 13742, 7298, 198, 43, 57404, 453, 30560, 220, 20, 13742, 7298, 198, 4839, 13891, 12566, 804, 1447, 2121, 5565, 258, 220, 23, 16, 13742, 7298, 198, 1655, 269, 85, 559, 14768, 220, 19, 15, 13742, 7298, 198, 43, 57404, 453, 30560, 220, 16, 15, 13742, 7298, 198, 4839, 13891, 94352, 25, 386, 6019, 72, 7786, 11, 8104, 1736, 1560, 5559, 84895, 271, 4839, 13891, 15180, 25, 83341, 271, 4839, 13891, 38297, 1447, 12480, 705, 448, 17773, 19784, 709, 263, 304, 220, 17, 5555, 198, 23526, 29910, 438, 30931, 198, 30098, 63808, 13495, 323, 1895, 894, 501, 4344, 198, 12480, 5239, 38297, 510, 12480, 705, 448, 17773, 19784, 709, 263, 304, 220, 17, 5555, 198, 2, 5430, 510, 28959, 32930, 3565, 504, 279, 3364, 624, 2, 9258, 4718, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 4913, 26485, 6112, 788, 330, 50, 37995, 1913, 746, 315, 10083, 8536, 41164, 2090, 497, 330, 26485, 51354, 788, 4383, 82, 37995, 40980, 497, 330, 1291, 1424, 23078, 497, 330, 78243, 529, 2645, 497, 330, 68192, 33115, 307, 21925, 497, 330, 811, 2798, 35681, 7914, 330, 26485, 27251, 788, 4383, 32, 220, 20, 20, 4666, 6284, 8778, 8720, 10449, 448, 10968, 40980, 315, 23078, 304, 1059, 1290, 1424, 10465, 330, 7941, 1030, 264, 3840, 315, 62208, 323, 17071, 33115, 307, 21925, 10465, 330, 3862, 572, 902, 5089, 2997, 3840, 476, 3681, 63808, 48809, 1189, 1125, 330, 26485, 11847, 788, 330, 12120, 497, 330, 26485, 47377, 788, 61753, 2996, 3142, 788, 330, 20, 20, 4666, 6284, 8778, 497, 330, 2996, 1819, 788, 330, 8987, 2220, 11749, 14345, 5212, 2996, 3142, 788, 330, 78243, 529, 2645, 497, 330, 2996, 1819, 788, 330, 67, 55307, 14345, 5212, 2996, 3142, 788, 330, 68192, 33115, 307, 21925, 497, 330, 2996, 1819, 788, 330, 67, 55307, 9207, 23439, 73594, 151645, 198]\n","inputs:\n","<|im_start|>system\n","You are a professional NLP data parser.\n","Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\n","Do not generate any introduction or conclusion.<|im_end|>\n","<|im_start|>user\n","# Story:\n","Admission Date: [2845-11-10] \n","Discharge Date: [2845-12-15]\n","Date of Birth: [2768-3-20]\n","Sex: F\n","Service: NEURO\n","\n","Chief Complaint: Sudden loss of consciousness, confusion\n","\n","Major Surgical or Invasive Procedure: Craniotomy with excision of intracranial mass\n","History of Present Illness: A 42-year-old female presented with sudden loss of consciousness and confusion for the past two days. Patient has a history of headaches and recent onset of weakness on the right side of the body. An MRI scan revealed a large intracranial mass, and the patient underwent emergency craniotomy for tumor resection.\n","Allergies: NKDA\n","\n","Past Medical History: Hypertension, Hyperlipidemia\n","Social History: Non-smoker, non-drinker, office worker, married, with two children\n","\n","Family History: Mother with history of hypertension\n","\n","Physical Exam:\n","General: Well-developed, well-nourished female in no acute distress\n","Vital signs: BP 122/75 mmHg, HR 74 bpm, RR 16 breaths per minute, Temp 98.6°F, SpO2 98% on room air\n","Cardiovascular: Regular rate and rhythm, no murmurs, rubs, or gallops\n","Respiratory: Clear to auscultation bilaterally\n","Abdomen: Soft, non-tender, non-distended\n","Extremities: No edema, pulses intact\n","\n","Pertinent Results:\n","\n","ECG: Normal\n","Cardiac enzymes: Elevated troponin levels\n","Chest X-ray: No acute cardiopulmonary abnormalities\n","Blood Test: White blood cell count 12,000/mm3 with 75% neutrophils, Hemoglobin 12.5 g/dL, and platelet count 150,000/mm3\n","\n","Medications on Admission:\n","\n","Aspirin 81 mg daily\n","Atorvastatin 40 mg daily\n","Lisinopril 5 mg daily\n","Discharge Medications:\n","\n","Aspirin 81 mg daily\n","Atorvastatin 40 mg daily\n","Lisinopril 10 mg daily\n","Discharge Diagnosis: Meningioma, Status post craniotomy\n","\n","Discharge Condition: Stable\n","\n","Discharge Instructions:\n","\n","Follow up with neurosurgeon in 2 weeks\n","Continue medications as prescribed\n","Monitor neurological symptoms and report any new changes\n","Follow-up Instructions:\n","Follow up with neurosurgeon in 2 weeks\n","# Task:\n","Extract structured details from the story.\n","# Output JSON:\n","```json<|im_end|>\n","<|im_start|>assistant\n","```json\n","{\"story_title\": \"Sudden Onset of Right Hand Weakness\", \"story_keywords\": [\"sudden onset\", \"right hand weakness\", \"hypertension\", \"hyperlipidemia\", \"neurology\"], \"story_summary\": [\"A 55-year-old female patient presented with sudden onset of weakness in her right hand.\", \"She had a history of hypertension and hyperlipidemia.\", \"There was no significant family history or previous neurological illnesses.\"], \"story_category\": \"health\", \"story_entities\": [{\"entity_value\": \"55-year-old female\", \"entity_type\": \"person-female\"}, {\"entity_value\": \"hypertension\", \"entity_type\": \"disease\"}, {\"entity_value\": \"hyperlipidemia\", \"entity_type\": \"disease\"}]}\n","```<|im_end|>\n","\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 4913, 26485, 6112, 788, 330, 50, 37995, 1913, 746, 315, 10083, 8536, 41164, 2090, 497, 330, 26485, 51354, 788, 4383, 82, 37995, 40980, 497, 330, 1291, 1424, 23078, 497, 330, 78243, 529, 2645, 497, 330, 68192, 33115, 307, 21925, 497, 330, 811, 2798, 35681, 7914, 330, 26485, 27251, 788, 4383, 32, 220, 20, 20, 4666, 6284, 8778, 8720, 10449, 448, 10968, 40980, 315, 23078, 304, 1059, 1290, 1424, 10465, 330, 7941, 1030, 264, 3840, 315, 62208, 323, 17071, 33115, 307, 21925, 10465, 330, 3862, 572, 902, 5089, 2997, 3840, 476, 3681, 63808, 48809, 1189, 1125, 330, 26485, 11847, 788, 330, 12120, 497, 330, 26485, 47377, 788, 61753, 2996, 3142, 788, 330, 20, 20, 4666, 6284, 8778, 497, 330, 2996, 1819, 788, 330, 8987, 2220, 11749, 14345, 5212, 2996, 3142, 788, 330, 78243, 529, 2645, 497, 330, 2996, 1819, 788, 330, 67, 55307, 14345, 5212, 2996, 3142, 788, 330, 68192, 33115, 307, 21925, 497, 330, 2996, 1819, 788, 330, 67, 55307, 9207, 23439, 73594, 151645, 198]\n","labels:\n","```json\n","{\"story_title\": \"Sudden Onset of Right Hand Weakness\", \"story_keywords\": [\"sudden onset\", \"right hand weakness\", \"hypertension\", \"hyperlipidemia\", \"neurology\"], \"story_summary\": [\"A 55-year-old female patient presented with sudden onset of weakness in her right hand.\", \"She had a history of hypertension and hyperlipidemia.\", \"There was no significant family history or previous neurological illnesses.\"], \"story_category\": \"health\", \"story_entities\": [{\"entity_value\": \"55-year-old female\", \"entity_type\": \"person-female\"}, {\"entity_value\": \"hypertension\", \"entity_type\": \"disease\"}, {\"entity_value\": \"hyperlipidemia\", \"entity_type\": \"disease\"}]}\n","```<|im_end|>\n","\n","num_proc must be <= 9. Reducing num_proc to 9 for dataset of size 9.\n","Running tokenizer on dataset (num_proc=9): 100% 9/9 [00:01<00:00,  4.52 examples/s]\n","eval example:\n","input_ids:\n","[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 15106, 510, 2589, 2728, 2631, 25, 508, 17, 23, 19, 21, 12, 19, 12, 17, 15, 60, 4093, 13891, 2631, 25, 508, 17, 23, 19, 21, 12, 20, 12, 16, 15, 921, 1916, 315, 35027, 25, 508, 17, 22, 24, 20, 12, 23, 12, 16, 23, 60, 6695, 25, 434, 198, 1860, 25, 7856, 52, 33439, 1511, 8738, 56, 271, 65807, 67438, 25, 328, 37995, 40980, 315, 46746, 11, 21340, 11, 323, 23078, 389, 825, 3108, 315, 279, 2487, 198, 34475, 70112, 476, 758, 77034, 44446, 25, 4553, 5559, 84895, 369, 59645, 458, 12559, 1047, 76, 61856, 198, 13424, 315, 26642, 12516, 2090, 25, 220, 19, 22, 4666, 6284, 8778, 10449, 448, 10968, 40980, 315, 15386, 46746, 11, 21340, 11, 323, 23078, 389, 825, 3108, 315, 279, 2487, 13, 28924, 702, 264, 3840, 315, 62208, 323, 19578, 13, 7416, 72, 5696, 10457, 264, 59953, 3073, 59645, 458, 12559, 1047, 76, 11, 323, 1560, 5559, 84895, 572, 10660, 369, 564, 5654, 624, 2403, 2375, 550, 25, 70063, 6352, 271, 50113, 12939, 11099, 25, 38415, 529, 2645, 11, 72599, 198, 26317, 11099, 25, 11581, 4668, 10451, 11, 2477, 18643, 41112, 11, 19446, 438, 264, 11079, 11, 12224, 448, 1378, 2841, 271, 15192, 11099, 25, 21043, 448, 3840, 315, 62208, 271, 39253, 32310, 510, 15415, 25, 8325, 47961, 291, 11, 1632, 5279, 413, 3304, 8778, 304, 902, 29783, 34004, 198, 53, 2174, 11929, 25, 29067, 220, 16, 17, 15, 14, 23, 15, 9465, 39, 70, 11, 22299, 220, 23, 23, 97724, 11, 43398, 220, 17, 17, 11486, 82, 817, 9383, 11, 19944, 220, 24, 23, 13, 21, 58472, 11, 3089, 46, 17, 220, 24, 20, 4, 389, 3054, 3720, 198, 5770, 815, 32845, 25, 28800, 4379, 323, 36290, 11, 902, 86551, 1723, 11, 10273, 82, 11, 476, 15369, 53689, 198, 1061, 5565, 5269, 25, 12023, 311, 9421, 92013, 367, 20316, 962, 745, 198, 5830, 5600, 268, 25, 24079, 11, 2477, 2385, 1659, 11, 2477, 87259, 2883, 198, 6756, 1826, 1361, 25, 2308, 1578, 9176, 11, 65457, 34439, 271, 47, 529, 13847, 18099, 1447, 7498, 38, 25, 18437, 75814, 36290, 198, 34, 6402, 1599, 29530, 25, 2308, 29783, 72051, 453, 360, 54792, 74715, 198, 1162, 8569, 25, 431, 7564, 3073, 59645, 458, 12559, 1047, 76, 73059, 2337, 14829, 198, 51486, 7032, 25, 96593, 6543, 52460, 64, 46403, 320, 33, 1861, 8, 323, 40429, 6056, 82234, 5866, 271, 13310, 292, 804, 389, 62346, 1447, 44, 16347, 482, 88318, 220, 17, 13742, 16824, 2793, 198, 3357, 32395, 983, 258, 220, 16, 15, 15, 13742, 16824, 2793, 198, 37, 2798, 6998, 577, 220, 19, 15, 13742, 16824, 2793, 198, 2304, 3334, 26422, 54789, 482, 38595, 220, 20, 15, 19223, 70, 12932, 7298, 198, 4839, 13891, 12566, 804, 1447, 44, 16347, 482, 88318, 220, 17, 13742, 12932, 1449, 220, 19, 4115, 438, 4362, 198, 3357, 32395, 983, 258, 220, 16, 15, 15, 13742, 12932, 10917, 7298, 198, 37, 2798, 6998, 577, 220, 19, 15, 13742, 12932, 3055, 7298, 198, 2304, 3334, 26422, 54789, 482, 38595, 220, 20, 15, 19223, 70, 12932, 3055, 7298, 198, 4839, 13891, 94352, 25, 431, 7564, 3073, 59645, 458, 12559, 1047, 76, 11, 1345, 19289, 62208, 271, 4839, 13891, 15180, 25, 83341, 271, 4839, 13891, 38297, 1447, 12480, 705, 448, 17773, 839, 380, 304, 220, 17, 5555, 198, 23526, 29910, 438, 30931, 198, 30098, 6543, 7262, 323, 1895, 894, 501, 13495, 198, 12480, 5239, 38297, 510, 12480, 705, 448, 17773, 839, 380, 304, 220, 17, 5555, 198, 2, 5430, 510, 28959, 32930, 3565, 504, 279, 3364, 624, 2, 9258, 4718, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 4913, 26485, 6112, 788, 330, 50, 37995, 1913, 746, 315, 10083, 8536, 41164, 2090, 497, 330, 26485, 51354, 788, 4383, 82, 37995, 40980, 497, 330, 1291, 1424, 23078, 497, 330, 78243, 529, 2645, 497, 330, 68192, 33115, 307, 21925, 497, 330, 811, 2798, 35681, 7914, 330, 26485, 27251, 788, 4383, 32, 220, 20, 20, 4666, 6284, 8778, 8720, 10449, 448, 10968, 40980, 315, 23078, 304, 1059, 1290, 1424, 10465, 330, 7941, 1030, 264, 3840, 315, 62208, 323, 17071, 33115, 307, 21925, 10465, 330, 3862, 572, 902, 5089, 2997, 3840, 476, 3681, 63808, 48809, 1189, 1125, 330, 26485, 11847, 788, 330, 12120, 497, 330, 26485, 47377, 788, 61753, 2996, 3142, 788, 330, 20, 20, 4666, 6284, 8778, 497, 330, 2996, 1819, 788, 330, 8987, 2220, 11749, 14345, 5212, 2996, 3142, 788, 330, 78243, 529, 2645, 497, 330, 2996, 1819, 788, 330, 67, 55307, 14345, 5212, 2996, 3142, 788, 330, 68192, 33115, 307, 21925, 497, 330, 2996, 1819, 788, 330, 67, 55307, 9207, 23439, 73594, 151645, 198]\n","inputs:\n","<|im_start|>system\n","You are a professional NLP data parser.\n","Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\n","Do not generate any introduction or conclusion.<|im_end|>\n","<|im_start|>user\n","# Story:\n","Admission Date: [2846-4-20] Discharge Date: [2846-5-10]\n","Date of Birth: [2795-8-18] Sex: F\n","Service: NEUROSURGERY\n","\n","Chief Complaint: Sudden onset of headache, confusion, and weakness on one side of the body\n","Major Surgical or Invasive Procedure: Craniotomy for cerebral aneurysm clipping\n","History of Present Illness: 47-year-old female presented with sudden onset of severe headache, confusion, and weakness on one side of the body. Patient has a history of hypertension and smoking. Angiography revealed a ruptured cerebral aneurysm, and craniotomy was performed forclipping.\n","Allergies: NKDA\n","\n","Past Medical History: Hypertension, Smoking\n","Social History: Non-smoker, non-drinker, employed as a teacher, married with two children\n","\n","Family History: Mother with history of hypertension\n","\n","Physical Exam:\n","General: Well-developed, well-nourished female in no acute distress\n","Vital signs: BP 120/80 mmHg, HR 88 bpm, RR 22 breaths per minute, Temp 98.6°F, SpO2 95% on room air\n","Cardiovascular: Regular rate and rhythm, no murmurs, rubs, or gallops\n","Respiratory: Clear to auscultation bilaterally\n","Abdomen: Soft, non-tender, non-distended\n","Extremities: No edema, pulses intact\n","\n","Pertinent Results:\n","\n","ECG: Normal sinus rhythm\n","Chest X-ray: No acute cardiopulmonary abnormalities\n","CT scan: Ruptured cerebral aneurysm clipped during surgery\n","Blood tests: Elevated blood urea nitrogen (BUN) and serum creatinine levels\n","\n","Medications on Admission:\n","\n","Morphine sulfate 2 mg IV stat\n","Phenytoin 100 mg IV stat\n","Furosemide 40 mg IV stat\n","Levothyroxine sodium 50 mcg PO daily\n","Discharge Medications:\n","\n","Morphine sulfate 2 mg PO every 4 hours as needed\n","Phenytoin 100 mg PO twice daily\n","Furosemide 40 mg PO once daily\n","Levothyroxine sodium 50 mcg PO once daily\n","Discharge Diagnosis: Ruptured cerebral aneurysm, Severe hypertension\n","\n","Discharge Condition: Stable\n","\n","Discharge Instructions:\n","\n","Follow up with neurologist in 2 weeks\n","Continue medications as prescribed\n","Monitor blood pressure and report any new symptoms\n","Follow-up Instructions:\n","Follow up with neurologist in 2 weeks\n","# Task:\n","Extract structured details from the story.\n","# Output JSON:\n","```json<|im_end|>\n","<|im_start|>assistant\n","```json\n","{\"story_title\": \"Sudden Onset of Right Hand Weakness\", \"story_keywords\": [\"sudden onset\", \"right hand weakness\", \"hypertension\", \"hyperlipidemia\", \"neurology\"], \"story_summary\": [\"A 55-year-old female patient presented with sudden onset of weakness in her right hand.\", \"She had a history of hypertension and hyperlipidemia.\", \"There was no significant family history or previous neurological illnesses.\"], \"story_category\": \"health\", \"story_entities\": [{\"entity_value\": \"55-year-old female\", \"entity_type\": \"person-female\"}, {\"entity_value\": \"hypertension\", \"entity_type\": \"disease\"}, {\"entity_value\": \"hyperlipidemia\", \"entity_type\": \"disease\"}]}\n","```<|im_end|>\n","\n","label_ids:\n","[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 4913, 26485, 6112, 788, 330, 50, 37995, 1913, 746, 315, 10083, 8536, 41164, 2090, 497, 330, 26485, 51354, 788, 4383, 82, 37995, 40980, 497, 330, 1291, 1424, 23078, 497, 330, 78243, 529, 2645, 497, 330, 68192, 33115, 307, 21925, 497, 330, 811, 2798, 35681, 7914, 330, 26485, 27251, 788, 4383, 32, 220, 20, 20, 4666, 6284, 8778, 8720, 10449, 448, 10968, 40980, 315, 23078, 304, 1059, 1290, 1424, 10465, 330, 7941, 1030, 264, 3840, 315, 62208, 323, 17071, 33115, 307, 21925, 10465, 330, 3862, 572, 902, 5089, 2997, 3840, 476, 3681, 63808, 48809, 1189, 1125, 330, 26485, 11847, 788, 330, 12120, 497, 330, 26485, 47377, 788, 61753, 2996, 3142, 788, 330, 20, 20, 4666, 6284, 8778, 497, 330, 2996, 1819, 788, 330, 8987, 2220, 11749, 14345, 5212, 2996, 3142, 788, 330, 78243, 529, 2645, 497, 330, 2996, 1819, 788, 330, 67, 55307, 14345, 5212, 2996, 3142, 788, 330, 68192, 33115, 307, 21925, 497, 330, 2996, 1819, 788, 330, 67, 55307, 9207, 23439, 73594, 151645, 198]\n","labels:\n","```json\n","{\"story_title\": \"Sudden Onset of Right Hand Weakness\", \"story_keywords\": [\"sudden onset\", \"right hand weakness\", \"hypertension\", \"hyperlipidemia\", \"neurology\"], \"story_summary\": [\"A 55-year-old female patient presented with sudden onset of weakness in her right hand.\", \"She had a history of hypertension and hyperlipidemia.\", \"There was no significant family history or previous neurological illnesses.\"], \"story_category\": \"health\", \"story_entities\": [{\"entity_value\": \"55-year-old female\", \"entity_type\": \"person-female\"}, {\"entity_value\": \"hypertension\", \"entity_type\": \"disease\"}, {\"entity_value\": \"hyperlipidemia\", \"entity_type\": \"disease\"}]}\n","```<|im_end|>\n","\n","[INFO|configuration_utils.py:696] 2025-04-12 04:34:01,834 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:768] 2025-04-12 04:34:01,835 >> Model config Qwen2Config {\n","  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.48.3\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|2025-04-12 04:34:01] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n","[INFO|modeling_utils.py:3904] 2025-04-12 04:34:01,880 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\n","[INFO|modeling_utils.py:1582] 2025-04-12 04:34:01,893 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n","[INFO|configuration_utils.py:1140] 2025-04-12 04:34:01,896 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"use_cache\": false\n","}\n","\n","[INFO|modeling_utils.py:4888] 2025-04-12 04:34:03,344 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n","\n","[INFO|modeling_utils.py:4896] 2025-04-12 04:34:03,344 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n","[INFO|configuration_utils.py:1095] 2025-04-12 04:34:03,525 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\n","[INFO|configuration_utils.py:1140] 2025-04-12 04:34:03,525 >> Generate config GenerationConfig {\n","  \"bos_token_id\": 151643,\n","  \"do_sample\": true,\n","  \"eos_token_id\": [\n","    151645,\n","    151643\n","  ],\n","  \"pad_token_id\": 151643,\n","  \"repetition_penalty\": 1.1,\n","  \"temperature\": 0.7,\n","  \"top_k\": 20,\n","  \"top_p\": 0.8\n","}\n","\n","[INFO|2025-04-12 04:34:03] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n","[INFO|2025-04-12 04:34:03] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n","[INFO|2025-04-12 04:34:03] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n","[INFO|2025-04-12 04:34:03] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n","[INFO|2025-04-12 04:34:03] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,o_proj,k_proj,v_proj,up_proj,down_proj,gate_proj\n","[INFO|2025-04-12 04:34:04] llamafactory.model.loader:143 >> trainable params: 36,929,536 || all params: 1,580,643,840 || trainable%: 2.3364\n","[INFO|trainer.py:741] 2025-04-12 04:34:04,276 >> Using auto half precision backend\n","[INFO|trainer.py:2369] 2025-04-12 04:34:04,752 >> ***** Running training *****\n","[INFO|trainer.py:2370] 2025-04-12 04:34:04,753 >>   Num examples = 73\n","[INFO|trainer.py:2371] 2025-04-12 04:34:04,753 >>   Num Epochs = 1\n","[INFO|trainer.py:2372] 2025-04-12 04:34:04,753 >>   Instantaneous batch size per device = 1\n","[INFO|trainer.py:2375] 2025-04-12 04:34:04,753 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n","[INFO|trainer.py:2376] 2025-04-12 04:34:04,753 >>   Gradient Accumulation steps = 2\n","[INFO|trainer.py:2377] 2025-04-12 04:34:04,753 >>   Total optimization steps = 36\n","[INFO|trainer.py:2378] 2025-04-12 04:34:04,756 >>   Number of trainable parameters = 36,929,536\n","[INFO|integration_utils.py:817] 2025-04-12 04:34:04,769 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n","\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n","{'loss': 0.9966, 'grad_norm': 1.4103772640228271, 'learning_rate': 4.5786740307563636e-05, 'epoch': 0.27}\n","{'loss': 0.148, 'grad_norm': 0.421121209859848, 'learning_rate': 2.5e-05, 'epoch': 0.55}\n","{'loss': 0.0091, 'grad_norm': 0.42057764530181885, 'learning_rate': 4.213259692436367e-06, 'epoch': 0.82}\n","100% 36/36 [05:17<00:00,  8.30s/it][INFO|trainer.py:3910] 2025-04-12 04:39:27,534 >> Saving model checkpoint to /gdrive/MyDrive/llm_Finetuning/models/checkpoint-36\n","[INFO|configuration_utils.py:696] 2025-04-12 04:39:27,790 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:768] 2025-04-12 04:39:27,791 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.48.3\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2491] 2025-04-12 04:39:28,316 >> tokenizer config file saved in /gdrive/MyDrive/llm_Finetuning/models/checkpoint-36/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2500] 2025-04-12 04:39:28,321 >> Special tokens file saved in /gdrive/MyDrive/llm_Finetuning/models/checkpoint-36/special_tokens_map.json\n","[INFO|trainer.py:2643] 2025-04-12 04:39:29,456 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 324.7004, 'train_samples_per_second': 0.225, 'train_steps_per_second': 0.111, 'train_loss': 0.3211939905045761, 'epoch': 0.99}\n","100% 36/36 [05:19<00:00,  8.87s/it]\n","[INFO|trainer.py:3910] 2025-04-12 04:39:29,464 >> Saving model checkpoint to /gdrive/MyDrive/llm_Finetuning/models/\n","[INFO|configuration_utils.py:696] 2025-04-12 04:39:29,691 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n","[INFO|configuration_utils.py:768] 2025-04-12 04:39:29,692 >> Model config Qwen2Config {\n","  \"architectures\": [\n","    \"Qwen2ForCausalLM\"\n","  ],\n","  \"attention_dropout\": 0.0,\n","  \"bos_token_id\": 151643,\n","  \"eos_token_id\": 151645,\n","  \"hidden_act\": \"silu\",\n","  \"hidden_size\": 1536,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 8960,\n","  \"max_position_embeddings\": 32768,\n","  \"max_window_layers\": 21,\n","  \"model_type\": \"qwen2\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 28,\n","  \"num_key_value_heads\": 2,\n","  \"rms_norm_eps\": 1e-06,\n","  \"rope_scaling\": null,\n","  \"rope_theta\": 1000000.0,\n","  \"sliding_window\": null,\n","  \"tie_word_embeddings\": true,\n","  \"torch_dtype\": \"bfloat16\",\n","  \"transformers_version\": \"4.48.3\",\n","  \"use_cache\": true,\n","  \"use_sliding_window\": false,\n","  \"vocab_size\": 151936\n","}\n","\n","[INFO|tokenization_utils_base.py:2491] 2025-04-12 04:39:30,182 >> tokenizer config file saved in /gdrive/MyDrive/llm_Finetuning/models/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2500] 2025-04-12 04:39:30,186 >> Special tokens file saved in /gdrive/MyDrive/llm_Finetuning/models/special_tokens_map.json\n","***** train metrics *****\n","  epoch                    =     0.9863\n","  total_flos               =   480737GF\n","  train_loss               =     0.3212\n","  train_runtime            = 0:05:24.70\n","  train_samples_per_second =      0.225\n","  train_steps_per_second   =      0.111\n","Figure saved at: /gdrive/MyDrive/llm_Finetuning/models/training_loss.png\n","[WARNING|2025-04-12 04:39:32] llamafactory.extras.ploting:148 >> No metric eval_news_finetune_val_loss to plot.\n","[WARNING|2025-04-12 04:39:32] llamafactory.extras.ploting:148 >> No metric eval_news_finetune_val_accuracy to plot.\n","[INFO|trainer.py:4226] 2025-04-12 04:39:32,292 >> \n","***** Running Evaluation *****\n","[INFO|trainer.py:4228] 2025-04-12 04:39:32,292 >>   Num examples = 9\n","[INFO|trainer.py:4231] 2025-04-12 04:39:32,292 >>   Batch size = 1\n","100% 9/9 [00:10<00:00,  1.19s/it]\n","***** eval metrics *****\n","  epoch                                     =     0.9863\n","  eval_news_finetune_val_loss               =     0.0037\n","  eval_news_finetune_val_runtime            = 0:00:12.08\n","  eval_news_finetune_val_samples_per_second =      0.745\n","  eval_news_finetune_val_steps_per_second   =      0.745\n","[INFO|modelcard.py:449] 2025-04-12 04:39:44,397 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: You can sync this run to the cloud by running:\n","\u001b[1;34mwandb\u001b[0m: \u001b[1mwandb sync /content/LLaMA-Factory/wandb/offline-run-20250412_043409-jrjvkei2\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/offline-run-20250412_043409-jrjvkei2/logs\u001b[0m\n"]}],"source":["!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Flj_wAWJN3D8"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":111,"status":"ok","timestamp":1744431651390,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"lmBnQEJwOloZ","outputId":"c9bd27b9-67b4-41d0-94fa-f382fc8585bb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Apr 12 04:21:52 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   73C    P0             33W /   70W |    6160MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["! nvidia-smi\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"elapsed":89,"status":"error","timestamp":1744633657012,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"7KDynM7fUaLk","outputId":"f88054f9-a3c7-4c80-bb10-3cb768db58d5"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-5cd34366f550>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfinetuned_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/gdrive/MyDrive/llm_Finetuning/models/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetuned_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["finetuned_model_id = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","model.load_adapter(finetuned_model_id)"]},{"cell_type":"markdown","metadata":{"id":"q_HKDWwIoTjm"},"source":["##How to organize the full Code After the model created and saved in my drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2526,"status":"ok","timestamp":1744470590477,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"CUEFq1VqjB_V","outputId":"6bf0bd1a-ef9c-40d0-a494-69531db90e16"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Flask-Cors\n","  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n","Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from Flask-Cors) (3.1.0)\n","Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from Flask-Cors) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->Flask-Cors) (1.9.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->Flask-Cors) (3.0.2)\n","Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n","Installing collected packages: Flask-Cors\n","Successfully installed Flask-Cors-5.0.1\n"]}],"source":["!pip install Flask-Cors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1134073,"status":"ok","timestamp":1744475601807,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"-qNUFBRfomPP","outputId":"edac683e-61ff-4b6a-b5c4-9e8492b83d2a"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Ngrok Tunnel established at: https://eaf4-35-229-251-39.ngrok-free.app/process-text\n","\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:8000\n"," * Running on http://172.28.0.12:8000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [12/Apr/2025 16:16:05] \"OPTIONS /process-text HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [12/Apr/2025 16:16:05] \"\u001b[31m\u001b[1mPOST /process-text HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [12/Apr/2025 16:16:37] \"OPTIONS /process-text HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [12/Apr/2025 16:16:37] \"\u001b[31m\u001b[1mPOST /process-text HTTP/1.1\u001b[0m\" 400 -\n","INFO:werkzeug:127.0.0.1 - - [12/Apr/2025 16:16:59] \"OPTIONS /process-text HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [12/Apr/2025 16:17:00] \"\u001b[31m\u001b[1mPOST /process-text HTTP/1.1\u001b[0m\" 400 -\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","from pyngrok import ngrok\n","import json\n","import logging\n","\n","# === Setup Logging for Console Output ===\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n","\n","# === Initialize Model and Tokenizer ===\n","def init_model_and_tokenizer(base_model_id, finetuned_model_path, device, torch_dtype):\n","    \"\"\"Initialize and load the model and tokenizer.\"\"\"\n","\n","    # Load the model\n","    model = AutoModelForCausalLM.from_pretrained(\n","        base_model_id,\n","        device_map=\"auto\",\n","        torch_dtype=torch_dtype\n","    )\n","    model.load_adapter(finetuned_model_path)\n","\n","    # Load the tokenizer\n","    tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","\n","    return model, tokenizer, device\n","\n","# === Setup Ngrok Tunnel ===\n","def init_ngrok():\n","    \"\"\"Set up Ngrok and return the public URL.\"\"\"\n","    ngrok.set_auth_token(\"2vcDYyYXQdybsp9IGWsryzLrhYy_4GVgdxmVwfdpT7vdTz3dL\")  # Add your ngrok token\n","    public_url = ngrok.connect(8000).public_url\n","    print(f\" Ngrok Tunnel established at: {public_url}/process-text\\n\")\n","    return public_url\n","\n","# === Model Response Generation ===\n","def generate_response(model, tokenizer, device, text):\n","    \"\"\"Generate response using the fine-tuned model.\"\"\"\n","\n","    # Move the model to the specified device (GPU or CPU)\n","    model.to(device)\n","\n","    # Prepare the input tensors and move them to the same device as the model\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    # Log the model input\n","    logging.info(f\"Model input text: {text}\")\n","\n","    try:\n","        # Generate the response from the model\n","        with torch.no_grad():\n","            generated_ids = model.generate(\n","                model_inputs.input_ids,\n","                max_new_tokens=1024,\n","                do_sample=False,\n","                top_k=None,\n","                temperature=None,\n","                top_p=None,\n","            )\n","\n","        # Check if the model has generated any output\n","        if generated_ids.size(1) > 0:\n","            logging.info(f\"Generated output IDs: {generated_ids}\")\n","\n","        # Trim the generated IDs to exclude the input part\n","        generated_ids_trimmed = [\n","            output_ids[len(input_ids):]\n","            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","        ]\n","\n","        # Decode the generated IDs back to text\n","        response_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=True)[0]\n","\n","        # Log the decoded response\n","        logging.info(f\"Decoded response text: {response_text}\")\n","        return response_text\n","    except Exception as e:\n","        logging.error(f\"Error during response generation: {str(e)}\")\n","        return \"An error occurred while generating the response.\"\n","\n","# === Flask API Setup ===\n","def create_flask_app(model, tokenizer, device):\n","    \"\"\"Create and return the Flask app.\"\"\"\n","    app = Flask(__name__)\n","    CORS(app)  # Enable CORS for all routes\n","\n","    @app.route('/process-text', methods=['POST'])\n","    def process_text():\n","        \"\"\"Handle text processing requests.\"\"\"\n","        data = request.get_json()\n","        story = data.get(\"story\", \"\").strip()\n","\n","        if not story:\n","            return jsonify({\"status\": \"error\", \"message\": \"Empty story received.\"}), 400\n","\n","        # Log the received text to console\n","        logging.info(f\"Received story: {story}\")\n","\n","        # Setup the details extraction with Pydantic schema\n","        details_extraction_messages = [\n","            {\n","                \"role\": \"system\",\n","                \"content\": \"\\n\".join([\n","                    \"You are an NLP data parser.\",\n","                    \"You will be provided with a text associated with a Pydantic scheme.\",\n","                    \"Generate the output in the same story language.\",\n","                    \"You have to extract JSON details from the text according to the Pydantic details.\",\n","                    \"Extract details as mentioned in the text.\",\n","                    \"Do not generate any introduction or conclusion.\"\n","                ])\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": \"\\n\".join([\n","                    \"## Story:\",\n","                    story,\n","                    \"\",\n","                    \"## Pydantic Details:\",\n","                    json.dumps(\n","                        NewsDetails.model_json_schema(), ensure_ascii=False\n","                    ),\n","                    \"\",\n","                    \"## Story Details:\",\n","                    \"```json\"\n","                ])\n","            }\n","        ]\n","\n","        # Format the story text according to the schema\n","        formatted_text = tokenizer.apply_chat_template(\n","            details_extraction_messages,\n","            tokenize=False,\n","            add_generation_prompt=True\n","        )\n","\n","        # Log the formatted text that will be passed to the model\n","        logging.info(f\"Formatted text for model: {formatted_text}\")\n","\n","        # Generate response from model\n","        response_text = generate_response(model, tokenizer, device, formatted_text)\n","\n","        # Log the output text to console\n","        logging.info(f\"Generated response: {response_text}\")\n","\n","        return jsonify({\"status\": \"success\", \"processed_text\": response_text})\n","\n","    return app\n","\n","# === Main Function to Run the Program ===\n","def main():\n","    base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","    finetuned_model_path = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","    device = \"cuda\"  # Or \"cpu\" if you want to run it on CPU\n","    torch_dtype = None  # Add the correct dtype if needed\n","\n","    # Initialize model, tokenizer, and device\n","    model, tokenizer, device = init_model_and_tokenizer(base_model_id, finetuned_model_path, device, torch_dtype)\n","\n","    # Initialize Ngrok and get public URL\n","    public_url = init_ngrok()\n","\n","    # Create and run the Flask app\n","    app = create_flask_app(model, tokenizer, device)\n","    app.run(host='0.0.0.0', port=8000)\n","\n","\n","if __name__ == '__main__':\n","    main()\n"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201966,"status":"ok","timestamp":1744661666661,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"},"user_tz":-120},"id":"pzjXr1ttw3ot","outputId":"1169a85b-3c79-4e37-8431-12aee789fc02"},"outputs":[{"output_type":"stream","name":"stdout","text":[" * Ngrok Tunnel: NgrokTunnel: \"https://a871-34-75-48-175.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:5000\n"," * Running on http://172.28.0.12:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:12:45] \"\u001b[33mOPTIONS /process-medical-report HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:13:31] \"OPTIONS /process-text HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:13:51] \"OPTIONS /process-text HTTP/1.1\" 200 -\n"]}],"source":["from flask import Flask, request, jsonify\n","from pydantic import BaseModel, Field\n","from typing import List, Literal\n","import json\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","import json_repair\n","from pyngrok import ngrok\n","\n","app = Flask(__name__)\n","\n","# Model Configuration\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","adapter_path = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","\n","# Initialize tokenizer and model\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",")\n","model.load_adapter(adapter_path)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","\n","# Pydantic Model Definitions\n","StoryCategory = Literal[\n","    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n","    \"health\", \"entertainment\", \"science\",\n","    \"not_specified\"\n","]\n","\n","EntityType = Literal[\n","    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n","    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n","]\n","\n","class Entity(BaseModel):\n","    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n","    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n","\n","class NewsDetails(BaseModel):\n","    story_title: str = Field(..., min_length=5, max_length=300,\n","                            description=\"A fully informative and SEO optimized title of the story.\")\n","    story_keywords: List[str] = Field(..., min_items=1,\n","                                    description=\"Relevant keywords associated with the story.\")\n","    story_summary: List[str] = Field(\n","        ..., min_items=1, max_items=5,\n","        description=\"Summarized key points about the story (1-5 points).\"\n","    )\n","    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n","    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n","                                        description=\"List of identified entities in the story.\")\n","\n","def parse_json(text):\n","    \"\"\"Robust JSON parsing with repair\"\"\"\n","    try:\n","        return json_repair.loads(text)\n","    except:\n","        return None\n","\n","def generate_response(story: str) -> str:\n","    print(f\"Input Story: {story}\")  # Debugging\n","\n","    details_extraction_messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are an NLP data parser.\",\n","                \"You will be provided with a text associated with a Pydantic scheme.\",\n","                \"Generate the output in the same story language.\",\n","                \"You have to extract JSON details from text according to the Pydantic details.\",\n","                \"Extract details as mentioned in text.\",\n","                \"Do not generate any introduction or conclusion.\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"\\n\".join([\n","                \"## Story:\",\n","                story.strip(),\n","                \"\",\n","                \"## Pydantic Details:\",\n","                json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n","                \"\",\n","                \"## Story Details:\",\n","                \"```json\"\n","            ])\n","        }\n","    ]\n","\n","    text = tokenizer.apply_chat_template(\n","        details_extraction_messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(\n","            model_inputs.input_ids,\n","            max_new_tokens=1024,\n","            do_sample=False, top_k=None, temperature=None, top_p=None,\n","        )\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    print(f\"Model Response: {response}\")  # Debugging\n","    return response\n","\n","@app.route(\"/process-text\", methods=[\"POST\"])\n","def process_text():\n","    data = request.get_json()\n","    if not data or 'story' not in data:\n","        return jsonify({\"status\": \"error\", \"message\": \"Missing 'story' field\"}), 400\n","\n","    story = data['story']\n","    print(f\"Flask received story: {story}\") #debug.\n","    raw_response = generate_response(story)\n","\n","    parsed_data = parse_json(raw_response)\n","    if not parsed_data:\n","        return jsonify({\n","            \"status\": \"error\",\n","            \"message\": \"Failed to parse model output\",\n","            \"raw_response\": raw_response\n","        }), 400\n","\n","    return jsonify({\n","        \"status\": \"success\",\n","        \"data\": parsed_data,\n","        \"raw_response\": raw_response\n","    })\n","\n","if __name__ == '__main__':\n","    public_url = ngrok.connect(5000)\n","    print(f\" * Ngrok Tunnel: {public_url}\")\n","    app.run(host='0.0.0.0', port=5000)\n"]},{"cell_type":"code","source":["!pip install pyngrok\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FpzdXQnQlO-8","executionInfo":{"status":"ok","timestamp":1744633675610,"user_tz":-120,"elapsed":2445,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"c503b141-c359-4f95-c5ad-b37a40edff26"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.3\n"]}]},{"cell_type":"code","source":["!ngrok config add-authtoken 2vcDYyYXQdybsp9IGWsryzLrhYy_4GVgdxmVwfdpT7vdTz3dL"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0gdzJ59n81p","executionInfo":{"status":"ok","timestamp":1744633678884,"user_tz":-120,"elapsed":608,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"cada4aef-d20d-433c-895e-a3983c8520b4"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]}]},{"cell_type":"code","source":["!pip install flask  pyngrok\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JE4lH3d_lA5i","executionInfo":{"status":"ok","timestamp":1744633683079,"user_tz":-120,"elapsed":2320,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"d8691d47-4c45-415f-c5cb-3dba2e7e8942"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n"]}]},{"cell_type":"code","source":["!pip install flask-cors\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xp31_mxsRoME","executionInfo":{"status":"ok","timestamp":1744633764027,"user_tz":-120,"elapsed":2519,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"6f155a28-4ff9-4cf1-b3c4-d2826de38842"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting flask-cors\n","  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n","Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.0)\n","Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.1.8)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n","Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n","Installing collected packages: flask-cors\n","Successfully installed flask-cors-5.0.1\n"]}]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"t85q3wQ36Ioj","executionInfo":{"status":"error","timestamp":1744661201264,"user_tz":-120,"elapsed":3969,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"c539c679-c0e1-46c2-a001-97f01c43ae43"},"outputs":[{"output_type":"stream","name":"stdout","text":[" * Running on NgrokTunnel: \"https://f364-34-75-48-175.ngrok-free.app\" -> \"http://localhost:8000\"\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.11.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.12.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.13.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.14.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.15.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.16.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.17.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.18.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.19.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.20.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.q_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.q_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.k_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.k_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.v_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.v_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.o_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.self_attn.o_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.gate_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.gate_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.up_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.up_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.down_proj.lora_A.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:2400: UserWarning: for model.layers.21.mlp.down_proj.lora_B.default.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n","  warnings.warn(\n"]},{"output_type":"error","ename":"ValueError","evalue":"weight is on the meta device, we need a `value` to put in on 0.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-d7920d7058f9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mfinetuned_model_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/gdrive/MyDrive/llm_Finetuning/models/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinetuned_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/peft.py\u001b[0m in \u001b[0;36mload_adapter\u001b[0;34m(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index, peft_config, adapter_state_dict, low_cpu_mem_usage, is_trainable, adapter_kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         ):\n\u001b[0;32m--> 276\u001b[0;31m             self._dispatch_accelerate_model(\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0mmax_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/peft.py\u001b[0m in \u001b[0;36m_dispatch_accelerate_model\u001b[0;34m(self, device_map, max_memory, offload_folder, offload_index)\u001b[0m\n\u001b[1;32m    521\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_split_module_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_split_module_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             )\n\u001b[0;32m--> 523\u001b[0;31m         dispatch_model(\n\u001b[0m\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/big_modeling.py\u001b[0m in \u001b[0;36mdispatch_model\u001b[0;34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[0m\n\u001b[1;32m    421\u001b[0m                 \u001b[0;31m# as we have no guarantee that safetensors' `file.get_tensor()` will always give the same pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         attach_align_device_hook_on_blocks(\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mexecution_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchild_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mchild_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{module_name}.{child_name}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mchild_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         attach_align_device_hook_on_blocks(\n\u001b[0m\u001b[1;32m    679\u001b[0m             \u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mexecution_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchild_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mchild_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{module_name}.{child_name}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mchild_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         attach_align_device_hook_on_blocks(\n\u001b[0m\u001b[1;32m    679\u001b[0m             \u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mexecution_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchild_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0mchild_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{module_name}.{child_name}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mchild_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         attach_align_device_hook_on_blocks(\n\u001b[0m\u001b[1;32m    679\u001b[0m             \u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0mexecution_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mattach_align_device_hook_on_blocks\u001b[0;34m(module, execution_device, offload, weights_map, offload_buffers, module_name, skip_keys, preload_module_classes, tied_params_map)\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mtied_params_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtied_params_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m         )\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0madd_hook_to_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         attach_execution_device_hook(\n\u001b[1;32m    638\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecution_device\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtied_params_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtied_params_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36madd_hook_to_module\u001b[0;34m(module, hook, append)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36minit_hook\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamed_module_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_submodules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m                 \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtied_params_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_params_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             self.original_devices = {\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{tensor_name} is on the meta device, we need a `value` to put in on {device}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: weight is on the meta device, we need a `value` to put in on 0."]}],"source":["from flask import Flask, request, jsonify\n","from flask_cors import CORS  # Importing CORS to enable cross-origin requests\n","from pydantic import BaseModel, Field\n","from typing import List, Literal\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from pyngrok import ngrok\n","\n","\n","app = Flask(__name__)\n","CORS(app)\n","\n","\n","\n","# Setting up Ngrok tunnel for public access (if needed)\n","public_url = ngrok.connect(8000)\n","print(f\" * Running on {public_url}\")\n","\n","# Entity Types with more medical relevance\n","EntityType = Literal[\n","    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n","    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"treatment\", \"medication\",\n","    \"procedure\", \"symptom\", \"body_part\", \"medical_condition\", \"diagnosis\",\n","    \"clinical_measurement\", \"lab_test\", \"artifact\", \"age\",  \"date\", \"not_specified\"\n","]\n","\n","# Report Categories\n","MedicalCategory = Literal[\n","    \"health\", \"medical\", \"pharmaceutical\", \"clinical_trial\", \"not_specified\"\n","]\n","\n","class Entity(BaseModel):\n","    entity_value: str = Field(..., description=\"Actual name or value of the entity.\")\n","    entity_type: EntityType = Field(..., description=\"Recognized entity type.\")\n","\n","class MedicalReportDetails(BaseModel):\n","    report_title: str = Field(..., min_length=5, max_length=300,\n","                              description=\"A precise and SEO-friendly medical report title.\")\n","    report_keywords: List[str] = Field(..., min_items=1,\n","                                       description=\"Relevant keywords describing the report.\")\n","    report_summary: List[str] = Field(..., min_items=1, max_items=5,\n","                                      description=\"Comprehensive clinical summary of the report.\")\n","    report_category: MedicalCategory = Field(..., description=\"The category of the medical report.\")\n","    report_entities: List[Entity] = Field(..., min_items=1, max_items=15,\n","                                          description=\"Extracted entities from the report,extract important entities from the report.\")\n","\n","# Model Setup\n","base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","device = \"cuda\"\n","torch_dtype = None\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch_dtype)\n","finetuned_model_id = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","model.load_adapter(finetuned_model_id)\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","\n","def extract_medical_details(report):\n","    prompt_messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are an expert medical NLP parser.\",\n","                \"You will receive a raw medical report in natural language.\",\n","                \"Your task is to extract structured information as per the provided Pydantic schema.\",\n","                \"The output must strictly follow JSON format and match the schema attributes.\",\n","                \"Keep the report's original language.\",\n","                \"Do not include explanations or headers—output only clean JSON.\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"\\n\".join([\n","                \"## Medical Report:\",\n","                report.strip(),\n","                \"\",\n","                \"## Pydantic Schema:\",\n","                json.dumps(MedicalReportDetails.model_json_schema(), ensure_ascii=False),\n","                \"\",\n","                \"## Extracted JSON:\",\n","                \"```json\"\n","            ])\n","        }\n","    ]\n","\n","    text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024)\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","    try:\n","        response_json = json.loads(response)\n","        return {\"status\": \"success\", \"data\": response_json}\n","    except json.JSONDecodeError:\n","        return {\"error\": \"Failed to parse JSON output.\"}, 500\n","\n","@app.route('/process-medical-report', methods=['POST'])\n","def process_medical_report():\n","    report_text = request.json.get('report', '')\n","    if not report_text:\n","        return jsonify({\"error\": \"Missing report content.\"}), 400\n","\n","    result = extract_medical_details(report_text)\n","    # Return the processed details\n","    return jsonify(result)\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=8000)\n"]},{"cell_type":"code","source":["from flask import Flask, request, jsonify\n","from flask_cors import CORS  # Importing CORS to enable cross-origin requests\n","from pydantic import BaseModel, Field\n","from typing import List, Literal\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from pyngrok import ngrok\n","\n","\n","app = Flask(__name__)\n","CORS(app)\n","\n","# Setting up Ngrok tunnel for public access (if needed)\n","public_url = ngrok.connect(8000)\n","print(f\" * Running on {public_url}\")\n","\n","# Entity Types with more medical relevance\n","EntityType = Literal[\n","    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n","    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"treatment\", \"medication\",\n","    \"procedure\", \"symptom\", \"body_part\", \"medical_condition\", \"diagnosis\",\n","    \"clinical_measurement\", \"lab_test\", \"artifact\", \"age\",  \"date\", \"not_specified\"\n","]\n","\n","# Report Categories\n","MedicalCategory = Literal[\n","    \"health\", \"medical\", \"pharmaceutical\", \"clinical_trial\", \"not_specified\"\n","]\n","\n","class Entity(BaseModel):\n","    entity_value: str = Field(..., description=\"Actual name or value of the entity.\")\n","    entity_type: EntityType = Field(..., description=\"Recognized entity type.\")\n","\n","class MedicalReportDetails(BaseModel):\n","    report_title: str = Field(..., min_length=5, max_length=300,\n","                              description=\"A precise and SEO-friendly medical report title.\")\n","    report_keywords: List[str] = Field(..., min_items=1,\n","                                       description=\"Relevant keywords describing the report.\")\n","    report_summary: List[str] = Field(..., min_items=1, max_items=5,\n","                                      description=\"Comprehensive clinical summary of the report.\")\n","    report_category: MedicalCategory = Field(..., description=\"The category of the medical report.\")\n","    report_entities: List[Entity] = Field(..., min_items=1, max_items=15,\n","                                          description=\"Extracted entities from the report,extract important entities from the report.\")\n","\n","# Model Setup\n","base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","device = \"cuda\"\n","torch_dtype = None\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch_dtype)\n","finetuned_model_id = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","model.load_adapter(finetuned_model_id)\n","\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","\n","def extract_medical_details(report):\n","    prompt_messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are an expert medical NLP parser.\",\n","                \"You will receive a raw medical report in natural language.\",\n","                \"Your task is to extract structured information as per the provided Pydantic schema.\",\n","                \"The output must strictly follow JSON format and match the schema attributes.\",\n","                \"Keep the report's original language.\",\n","                \"Do not include explanations or headers—output only clean JSON.\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"\\n\".join([\n","                \"## Medical Report:\",\n","                report.strip(),\n","                \"\",\n","                \"## Pydantic Schema:\",\n","                json.dumps(MedicalReportDetails.model_json_schema(), ensure_ascii=False),\n","                \"\",\n","                \"## Extracted JSON:\",\n","                \"```json\"\n","            ])\n","        }\n","    ]\n","\n","    text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024)\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","    try:\n","        response_json = json.loads(response)\n","        return {\"status\": \"success\", \"data\": response_json}\n","    except json.JSONDecodeError:\n","        return {\"error\": \"Failed to parse JSON output.\"}, 500\n","\n","@app.route('/process-medical-report', methods=['POST'])\n","def process_medical_report():\n","    report_text = request.json.get('report', '')\n","    if not report_text:\n","        return jsonify({\"error\": \"Missing report content.\"}), 400\n","\n","    result = extract_medical_details(report_text)\n","    # Return the processed details\n","    return jsonify(result)\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=8000)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYKpiVNjdazR","executionInfo":{"status":"ok","timestamp":1744661374931,"user_tz":-120,"elapsed":63300,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"a13e2244-289a-43cb-edd7-6a1883c0fba1"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Running on NgrokTunnel: \"https://0047-34-75-48-175.ngrok-free.app\" -> \"http://localhost:8000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:8000\n"," * Running on http://172.28.0.12:8000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:10:08] \"OPTIONS /process-medical-report HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:10:16] \"POST /process-medical-report HTTP/1.1\" 200 -\n"]}]},{"cell_type":"code","source":["from flask import Flask, request, jsonify\n","from flask_cors import CORS\n","from pydantic import BaseModel, Field\n","from typing import List, Literal\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","from pyngrok import ngrok\n","import json\n","import requests  # For making web requests\n","from bs4 import BeautifulSoup  # For parsing HTML\n","\n","app = Flask(__name__)\n","CORS(app)\n","\n","# Setting up Ngrok tunnel for public access (if needed)\n","public_url = ngrok.connect(8000)\n","print(f\" * Running on {public_url}\")\n","\n","# Entity Types with more medical relevance\n","EntityType = Literal[\n","    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n","    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"treatment\", \"medication\",\n","    \"procedure\", \"symptom\", \"body_part\", \"medical_condition\", \"diagnosis\",\n","    \"clinical_measurement\", \"lab_test\", \"artifact\", \"age\",  \"date\", \"not_specified\"\n","]\n","\n","# Report Categories\n","MedicalCategory = Literal[\n","    \"health\", \"medical\", \"pharmaceutical\", \"clinical_trial\", \"not_specified\"\n","]\n","\n","class Entity(BaseModel):\n","    entity_value: str = Field(..., description=\"Actual name or value of the entity.\")\n","    entity_type: EntityType = Field(..., description=\"Recognized entity type.\")\n","\n","class MedicalReportDetails(BaseModel):\n","    report_title: str = Field(..., min_length=5, max_length=300,\n","                              description=\"A precise and SEO-friendly medical report title.\")\n","    report_keywords: List[str] = Field(..., min_items=1,\n","                                       description=\"Relevant keywords describing the report.\")\n","    report_summary: List[str] = Field(..., min_items=1, max_items=5,\n","                                      description=\"Comprehensive clinical summary of the report.\")\n","    report_category: MedicalCategory = Field(..., description=\"The category of the medical report.\")\n","    report_entities: List[Entity] = Field(..., min_items=1, max_items=15,\n","                                          description=\"Extracted entities from the report,extract important entities from the report.\")\n","\n","# Model Setup\n","base_model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n","device = \"cuda\"\n","torch_dtype = None\n","\n","model = AutoModelForCausalLM.from_pretrained(base_model_id, device_map=\"auto\", torch_dtype=torch_dtype)\n","#finetuned_model_id = \"/gdrive/MyDrive/llm_Finetuning/models/\"\n","#model.load_adapter(finetuned_model_id)\n","tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n","\n","def extract_medical_details(report):\n","    prompt_messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are an expert medical NLP parser.\",\n","                \"You will receive a raw medical report in natural language.\",\n","                \"Your task is to extract structured information as per the provided Pydantic schema.\",\n","                \"The output must strictly follow JSON format and match the schema attributes.\",\n","                \"Keep the report's original language.\",\n","                \"Do not include explanations or headers—output only clean JSON.\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"\\n\".join([\n","                \"## Medical Report:\",\n","                report.strip(),\n","                \"\",\n","                \"## Pydantic Schema:\",\n","                json.dumps(MedicalReportDetails.model_json_schema(), ensure_ascii=False),\n","                \"\",\n","                \"## Extracted JSON:\",\n","                \"```json\"\n","            ])\n","        }\n","    ]\n","\n","    text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024)\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","    try:\n","        response_json = json.loads(response)\n","        return {\"status\": \"success\", \"data\": response_json}\n","    except json.JSONDecodeError:\n","        return {\"error\": \"Failed to parse JSON output.\"}, 500\n","\n","def search_web(query):\n","    \"\"\"Search the web for the given query and return relevant information.\"\"\"\n","    try:\n","        # This is a simple implementation using requests and BeautifulSoup\n","        # In production, you might want to use a proper search API like Google Custom Search API\n","        search_url = f\"https://www.google.com/search?q={requests.utils.quote(query)}\"\n","        headers = {\n","            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n","        }\n","\n","        response = requests.get(search_url, headers=headers)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        # Extract the first few search results\n","        results = []\n","        for g in soup.find_all('div', class_='tF2Cxc'):\n","            anchor = g.find('a')\n","            title = g.find('h3').text if g.find('h3') else \"No title\"\n","            link = anchor['href'] if anchor else \"No link\"\n","            description = g.find('div', class_='IsZvec').text if g.find('div', class_='IsZvec') else \"No description\"\n","\n","            results.append({\n","                \"title\": title,\n","                \"link\": link,\n","                \"description\": description\n","            })\n","            if len(results) >= 3:  # Limit to 3 results\n","                break\n","\n","        return results\n","    except Exception as e:\n","        return {\"error\": str(e)}\n","\n","def generate_answer_from_web_results(question, web_results):\n","    \"\"\"Generate an answer based on web search results.\"\"\"\n","    context = \"\\n\".join([f\"Source {i+1}: {result['description']}\" for i, result in enumerate(web_results)])\n","\n","    prompt_messages = [\n","        {\n","            \"role\": \"system\",\n","            \"content\": \"\\n\".join([\n","                \"You are a helpful AI assistant that answers questions based on web search results.\",\n","                \"You will receive a question and some web search results.\",\n","                \"Your task is to synthesize an accurate answer based on the search results.\",\n","                \"If the search results don't contain the answer, say you don't know.\",\n","                \"Always cite your sources when possible.\"\n","            ])\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": \"\\n\".join([\n","                f\"## Question:\",\n","                question,\n","                \"\",\n","                f\"## Web Search Results:\",\n","                context,\n","                \"\",\n","                \"## Answer:\"\n","            ])\n","        }\n","    ]\n","\n","    text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=1024)\n","\n","    generated_ids = [\n","        output_ids[len(input_ids):]\n","        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    return response\n","\n","@app.route('/process-medical-report', methods=['POST'])\n","def process_medical_report():\n","    report_text = request.json.get('report', '')\n","    if not report_text:\n","        return jsonify({\"error\": \"Missing report content.\"}), 400\n","\n","    result = extract_medical_details(report_text)\n","    return jsonify(result)\n","\n","@app.route('/answer-question', methods=['POST'])\n","def answer_question():\n","    question = request.json.get('question', '')\n","    if not question:\n","        return jsonify({\"error\": \"Missing question.\"}), 400\n","\n","    # Step 1: Search the web for the question\n","    web_results = search_web(question)\n","    if isinstance(web_results, dict) and \"error\" in web_results:\n","        return jsonify(web_results), 500\n","\n","    # Step 2: Generate an answer based on the web results\n","    answer = generate_answer_from_web_results(question, web_results)\n","\n","    return jsonify({\n","        \"status\": \"success\",\n","        \"question\": question,\n","        \"answer\": answer,\n","        \"sources\": web_results\n","    })\n","\n","if __name__ == '__main__':\n","    app.run(host='0.0.0.0', port=8000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGT9vWHTerCV","executionInfo":{"status":"ok","timestamp":1744661060016,"user_tz":-120,"elapsed":261761,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"5b4ea6cb-e2c3-4ff3-d32f-b5737ba181ad"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":[" * Running on NgrokTunnel: \"https://fd81-34-75-48-175.ngrok-free.app\" -> \"http://localhost:8000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:8000\n"," * Running on http://172.28.0.12:8000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","WARNING:pyngrok.process.ngrok:t=2025-04-14T20:01:05+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:01:53] \"OPTIONS /process-medical-report HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:02:02] \"POST /process-medical-report HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:02:24] \"OPTIONS /process-medical-report HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:02:34] \"POST /process-medical-report HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:02:47] \"OPTIONS /answer-question HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:02:52] \"POST /answer-question HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:03:58] \"OPTIONS /answer-question HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:04:02] \"POST /answer-question HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:04:25] \"OPTIONS /answer-question HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:04:29] \"POST /answer-question HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:04:55] \"OPTIONS /process-medical-report HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [14/Apr/2025 20:05:04] \"POST /process-medical-report HTTP/1.1\" 200 -\n"]}]},{"cell_type":"code","source":["import shutil\n","from google.colab import files\n","\n","# Zip the folder\n","shutil.make_archive('medical_nlp_api', 'zip', '/content')\n","\n","# Download the zipped folder\n","files.download('medical_nlp_api.zip')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"fpIk7D6aC3w0","executionInfo":{"status":"ok","timestamp":1744663671061,"user_tz":-120,"elapsed":23,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}},"outputId":"7934c622-1033-49e9-8634-26cd4f83aa13"},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_03185ef3-fce2-447d-bb7c-b096a4cdf081\", \"medical_nlp_api.zip\", 20237)"]},"metadata":{}}]},{"cell_type":"code","source":["!lsof -i :8000\n"],"metadata":{"id":"M5FrB8r5i-Hb","executionInfo":{"status":"ok","timestamp":1744655116051,"user_tz":-120,"elapsed":103,"user":{"displayName":"marwa elsayed","userId":"10601944389375105673"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0VII7GsUau9S"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyN0i3vCctDE9JVcmURG3g1u"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}